{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVnFBMETDaCG"
   },
   "source": [
    "Установка зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3TgZ5T9gOVaH",
    "outputId": "e553d7c3-24ff-4d69-d5f1-09a76f572a55"
   },
   "outputs": [],
   "source": [
    "# Установка дополнительных зависимостей\n",
    "#!pip install fiona\n",
    "#!pip install segmentation_models_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Gzuv2run9Yxa"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "import fiona\n",
    "import json\n",
    "import torch\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.utils import draw_segmentation_masks\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "from pprint import pprint\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oumBWPOlDuYz"
   },
   "source": [
    "Класс для загрузки датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IS7LtxhL8WWh"
   },
   "outputs": [],
   "source": [
    "class EyeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Класс датасета, организующий загрузку и получение изображений и соответствующих разметок\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_folder: str, transform = None):\n",
    "        self.class_ids = {\"vessel\": 1}\n",
    "\n",
    "        self.data_folder = data_folder\n",
    "        self.transform = transform\n",
    "        self._image_files = glob.glob(f\"{data_folder}/images512/*.png\")\n",
    "\n",
    "    @staticmethod\n",
    "    def read_image(path: str) -> np.ndarray:\n",
    "        image = cv2.imread(str(path), cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = np.array(image / 255, dtype=np.float32)\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        # Достаём имя файла по индексу\n",
    "        image_path = self._image_files[idx]\n",
    "\n",
    "        # Получаем соответствующий файл разметки\n",
    "        mask_path = image_path.replace(\"images512\", \"masks512\")\n",
    "        if os.path.isfile(mask_path):\n",
    "            image = self.read_image(image_path)\n",
    "            mask = self.read_image(mask_path)\n",
    "\n",
    "            sample = {'image': image,\n",
    "                      'mask': mask}\n",
    "\n",
    "            if self.transform is not None:\n",
    "                sample = self.transform(**sample)\n",
    "\n",
    "            return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._image_files)\n",
    "\n",
    "    # Метод для проверки состояния датасета\n",
    "    def make_report(self):\n",
    "        \n",
    "        reports = []\n",
    "        if (not self.data_folder):\n",
    "            reports.append(\"Путь к датасету не указан\")\n",
    "        if (len(self._image_files) == 0):\n",
    "            reports.append(\"Изображения для распознавания не найдены\")\n",
    "        else:\n",
    "            reports.append(f\"Найдено {len(self._image_files)} изображений\")\n",
    "        cnt_images_without_masks = sum([1 - len(glob.glob(filepath.replace(\"images512\", \"masks512\"))) for filepath in self._image_files])\n",
    "        if cnt_images_without_masks > 0:\n",
    "            reports.append(f\"Найдено {cnt_images_without_masks} изображений без разметки\")\n",
    "        else:\n",
    "            reports.append(f\"Для всех изображений есть файл разметки\")\n",
    "        return reports\n",
    "\n",
    "\n",
    "class DatasetPart(Dataset):\n",
    "    \"\"\"\n",
    "    Обертка над классом датасета для его разбиения на части\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset: Dataset,\n",
    "                 indices: np.ndarray,\n",
    "                 transform: A.Compose = None):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        sample = self.dataset[self.indices[idx]]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(**sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7URcg7KlMqbV"
   },
   "outputs": [],
   "source": [
    "# Задаем преобразование изображений\n",
    "size = 512\n",
    "train_list = [A.LongestMaxSize(size, interpolation=cv2.INTER_CUBIC),\n",
    "              A.PadIfNeeded(size, size),\n",
    "              ToTensorV2(transpose_mask=True),\n",
    "              ]\n",
    "eval_list = [A.LongestMaxSize(size, interpolation=cv2.INTER_CUBIC),\n",
    "              A.PadIfNeeded(size, size),\n",
    "              ToTensorV2(transpose_mask=True)]\n",
    "\n",
    "transforms = {'train': A.Compose(train_list), 'test': A.Compose(eval_list)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5vqgjSAyMwT4"
   },
   "outputs": [],
   "source": [
    "# Инициализируем датасет\n",
    "dataset = EyeDataset(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ns9czMPFNxge",
    "outputId": "8bdd256d-e174-49a7-e34a-19b5c8bd319a"
   },
   "outputs": [],
   "source": [
    "# Проверим состояние загруженного датасета\n",
    "for msg in dataset.make_report():\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "LTTtIkoKJzLL",
    "outputId": "106777cb-958c-4fc3-bc72-b26e8ef812a2"
   },
   "outputs": [],
   "source": [
    "# Посмотрим на картинки из датасета\n",
    "\n",
    "fig, axs = plt.subplots(2, 4, figsize=(16,8))\n",
    "fig.suptitle(f'Изображения с масками {\" \"*105} Без масок', fontsize=14)\n",
    "\n",
    "\n",
    "for i, sample in zip(range(4), dataset):\n",
    "    transformed = transforms['test'](**sample)\n",
    "    image, mask = transformed[\"image\"], transformed[\"mask\"]\n",
    "\n",
    "    image_with_mask = draw_segmentation_masks((image.cpu() * 255).type(torch.uint8), mask.type(torch.bool))\n",
    "    image_with_mask = np.moveaxis(image_with_mask.cpu().numpy(), 0, -1)\n",
    "\n",
    "    axs[i // 2, (i % 2)].imshow(image_with_mask)\n",
    "    axs[i // 2, (i % 2)].axis('off')\n",
    "\n",
    "    image = np.moveaxis(image.cpu().numpy(), 0, -1)    \n",
    "    axs[i // 2, (i % 2)+2].imshow(image)\n",
    "    axs[i // 2, (i % 2)+2].axis('off')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cRfpg4grVx71",
    "outputId": "42199ba4-ed33-4fc9-b193-ed53111a5a20"
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "print(\"Обучающей выборки \" ,len(listdir(\"./data/images512/\")))\n",
    "print(\"Тестовой выборки \" ,len(listdir(\"./data/eye_test\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-glCq25BODdU"
   },
   "outputs": [],
   "source": [
    "# разделим датасет на тренировочный и валидационный, чтобы смотреть на качество\n",
    "train_indices, test_indices = train_test_split(range(len(dataset)), test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j4EHU18wPmqv",
    "outputId": "0e92dd25-9980-4e15-ca0d-a697a58884cd"
   },
   "outputs": [],
   "source": [
    "print(f\"Разбиение на train/test : {len(train_indices)}/{len(test_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zcp0qk5oPode"
   },
   "outputs": [],
   "source": [
    "# Разбиваем объект датасета на тренировачный и валидационный\n",
    "train_dataset = DatasetPart(dataset, train_indices, transform=transforms['train'])\n",
    "valid_dataset = DatasetPart(dataset, test_indices, transform=transforms['test'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukJo2MSwPsWL"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, 1,\n",
    "#                                    num_workers=2,\n",
    "                                   shuffle=True, drop_last=True)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, 1,\n",
    "#                                    num_workers=2,\n",
    "                                   shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qu_CF6TIP2ap"
   },
   "outputs": [],
   "source": [
    "def plot_history(train_history, val_history, title='loss'):\n",
    "    plt.figure()\n",
    "    plt.title('{}'.format(title))\n",
    "    plt.plot(train_history, label='train', zorder=1)\n",
    "    \n",
    "    points = np.array(val_history)\n",
    "    steps = list(range(0, len(train_history) + 1, int(len(train_history) / len(val_history))))[1:]\n",
    "    \n",
    "    plt.scatter(steps, val_history, marker='+', s=180, c='orange', label='val', zorder=2)\n",
    "    plt.xlabel('train steps')\n",
    "    \n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ac1V-EjHP8RT"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Callable, Iterator, Optional, Dict, Any\n",
    "from collections import defaultdict\n",
    "\n",
    "class UnetTrainer:\n",
    "    \"\"\"\n",
    "    Класс, реализующий обучение модели\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: nn.Module, optimizer: torch.optim.Optimizer,\n",
    "                 criterion: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
    "                 device: str, metric_functions: List[Tuple[str, Callable]] = [],\n",
    "                 epoch_number: int = 0,\n",
    "                 lr_scheduler: Optional[Any] = None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer  \n",
    "        self.criterion = criterion\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.metric_functions = metric_functions\n",
    "\n",
    "        self.epoch_number = epoch_number\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate_batch(self, val_iterator: Iterator, eval_on_n_batches: int) -> Optional[Dict[str, float]]:     \n",
    "        predictions = []\n",
    "        targets = []\n",
    "\n",
    "        losses = []\n",
    "        for real_batch_number in range(eval_on_n_batches):\n",
    "            try:\n",
    "                batch = next(val_iterator)\n",
    "\n",
    "                xs = batch['image'].to(self.device)\n",
    "                ys_true = batch['mask'].to(self.device)\n",
    "            except StopIteration:\n",
    "                if real_batch_number == 0:\n",
    "                    return None\n",
    "                else:\n",
    "                    break\n",
    "            ys_pred = self.model.eval()(xs)\n",
    "            loss = self.criterion(ys_pred, ys_true)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            predictions.append(ys_pred.cuda())\n",
    "            targets.append(ys_true.cuda())\n",
    "\n",
    "        predictions = torch.cat(predictions, dim=0)\n",
    "        targets = torch.cat(targets, dim=0)\n",
    "\n",
    "        metrics = {'loss': np.mean(losses)}\n",
    "\n",
    "        for metric_name, metric_fn in self.metric_functions:\n",
    "            metrics[metric_name] = metric_fn(predictions, targets).item()\n",
    "        return metrics\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, val_loader, eval_on_n_batches: int = 1) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Вычисление метрик для эпохи\n",
    "        \"\"\"\n",
    "        metrics_sum = defaultdict(float)\n",
    "        num_batches = 0\n",
    "\n",
    "        val_iterator = iter(val_loader)\n",
    "\n",
    "        while True:\n",
    "            batch_metrics = self.evaluate_batch(val_iterator, eval_on_n_batches)\n",
    "\n",
    "            if batch_metrics is None:\n",
    "                break\n",
    "\n",
    "            for metric_name in batch_metrics:\n",
    "                metrics_sum[metric_name] += batch_metrics[metric_name]\n",
    "\n",
    "            num_batches += 1\n",
    "\n",
    "        metrics = {}\n",
    "\n",
    "        for metric_name in metrics_sum:\n",
    "            metrics[metric_name] = metrics_sum[metric_name] / num_batches\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def fit_batch(self, train_iterator: Iterator, update_every_n_batches: int) -> Optional[Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Тренировка модели на одном батче\n",
    "        \"\"\"\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        predictions = []\n",
    "        targets = []\n",
    "\n",
    "        losses = []\n",
    "        for real_batch_number in range(update_every_n_batches):\n",
    "            try:\n",
    "                batch = next(train_iterator)\n",
    " \n",
    "                xs = batch['image'].to(self.device)\n",
    "                ys_true = batch['mask'].to(self.device)\n",
    "            except StopIteration:\n",
    "                if real_batch_number == 0:\n",
    "                    return None\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            ys_pred = self.model.train()(xs)\n",
    "\n",
    "            loss = self.criterion(ys_pred, ys_true)\n",
    "\n",
    "            (loss / update_every_n_batches).backward()\n",
    "          \n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            predictions.append(ys_pred.cpu())\n",
    "            targets.append(ys_true.cpu())\n",
    "            \n",
    "\n",
    "       \n",
    "        self.optimizer.step()\n",
    "\n",
    "        predictions = torch.cat(predictions, dim=0)\n",
    "        targets = torch.cat(targets, dim=0)\n",
    "\n",
    "        metrics = {'loss': np.mean(losses)}\n",
    "    \n",
    "        for metric_name, metric_fn in self.metric_functions:\n",
    "            metrics[metric_name] = metric_fn(predictions, targets).item()\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def fit_epoch(self, train_loader, update_every_n_batches: int = 1) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Одна эпоха тренировки модели\n",
    "        \"\"\"\n",
    "\n",
    "        metrics_sum = defaultdict(float)\n",
    "        num_batches = 8\n",
    "\n",
    "        train_iterator = iter(train_loader)\n",
    "\n",
    "        while True:\n",
    "            batch_metrics = self.fit_batch(train_iterator, update_every_n_batches)\n",
    "\n",
    "            if batch_metrics is None:\n",
    "                break\n",
    "\n",
    "            for metric_name in batch_metrics:\n",
    "                metrics_sum[metric_name] += batch_metrics[metric_name]\n",
    "\n",
    "            num_batches += 1\n",
    "\n",
    "        metrics = {}\n",
    "\n",
    "        for metric_name in metrics_sum:\n",
    "            metrics[metric_name] = metrics_sum[metric_name] / num_batches\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def fit(self, train_loader, num_epochs: int,\n",
    "            val_loader = None, update_every_n_batches: int = 1,\n",
    "            ) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Метод, тренирующий модель и вычисляющий метрики для каждой эпохи\n",
    "        \"\"\"\n",
    "\n",
    "        summary = defaultdict(list)\n",
    "\n",
    "        def save_metrics(metrics: Dict[str, float], postfix: str = '') -> None:\n",
    "          # Сохранение метрик в summary\n",
    "            nonlocal summary, self\n",
    "\n",
    "            for metric in metrics:\n",
    "                metric_name, metric_value = f'{metric}{postfix}', metrics[metric]\n",
    "\n",
    "                summary[metric_name].append(metric_value)\n",
    "\n",
    "        for _ in tqdm(range(num_epochs - self.epoch_number), initial=self.epoch_number, total=num_epochs):\n",
    "            self.epoch_number += 1\n",
    "\n",
    "            train_metrics = self.fit_epoch(train_loader, update_every_n_batches)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                save_metrics(train_metrics, postfix='_train')\n",
    "\n",
    "                if val_loader is not None:\n",
    "                    test_metrics = self.evaluate(val_loader)\n",
    "                    save_metrics(test_metrics, postfix='_test')\n",
    "\n",
    "            if self.lr_scheduler is not None:\n",
    "                self.lr_scheduler.step()\n",
    "\n",
    "        summary = {metric: np.array(summary[metric]) for metric in summary}\n",
    "\n",
    "        return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zltdk2x2VTzY"
   },
   "outputs": [],
   "source": [
    "# F1-мера\n",
    "class SoftDice:\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def __call__(self, predictions: List[Dict[str, torch.Tensor]],\n",
    "                 targets: List[Dict[str, torch.Tensor]]) -> torch.Tensor:\n",
    "        numerator = torch.sum(2 * predictions * targets)\n",
    "        denominator = torch.sum(predictions + targets)\n",
    "        return numerator / (denominator + self.epsilon)\n",
    "\n",
    "# Метрика полноты\n",
    "class Recall:\n",
    "    def __init__(self, epsilon=1e-8, b=1):\n",
    "        self.epsilon = epsilon\n",
    "        self.a = b*b\n",
    "\n",
    "    def __call__(self, predictions: List[Dict[str, torch.Tensor]],\n",
    "                 targets: List[Dict[str, torch.Tensor]]) -> torch.Tensor:\n",
    "        numerator = torch.sum(predictions * targets)\n",
    "        denominator = torch.sum(targets)\n",
    "\n",
    "        return numerator / (denominator + self.epsilon)\n",
    "\n",
    "# Метрика точности\n",
    "class Accuracy:\n",
    "    def __init__(self, epsilon=1e-8, b=1):\n",
    "        self.epsilon = epsilon\n",
    "        self.a = b*b\n",
    "\n",
    "    def __call__(self, predictions: list, targets: list) -> torch.Tensor:\n",
    "        numerator = torch.sum(targets)\n",
    "        denominator = torch.sum(predictions * targets)\n",
    "\n",
    "        return numerator / (denominator + self.epsilon)\n",
    "\n",
    "def make_metrics():\n",
    "    soft_dice = SoftDice()\n",
    "    recall = Recall()\n",
    "    _accuracy = Accuracy()\n",
    "\n",
    "    def exp_dice(pred, target):\n",
    "        return soft_dice(torch.exp(pred[:, 1:]), target[:, 1:])\n",
    "\n",
    "    def accuracy(pred, target):\n",
    "        return _accuracy(torch.exp(pred[:, 1:]), target[:, 1:])\n",
    "\n",
    "    def exp_recall(pred, target):\n",
    "        return recall(torch.exp(pred[:, 1:]), target[:, 1:])\n",
    "\n",
    "    return [('exp_dice', exp_dice),\n",
    "            ('accuracy', accuracy),\n",
    "            ('recall', exp_recall),\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DMSb-fY2P8hr"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87,
     "referenced_widgets": [
      "3e5fd46dd3d04e91a9eb09c157a64e75",
      "9fd271a089ed42cabb20e0040d043e72",
      "544f60a9a0674e4690b9cbbeb67e384e",
      "e92928dc60d34de087a5b29c86eadc3d",
      "0e92432f54b54b9abc2ba7636fbf1f77",
      "327c8a838df5445d8a8a5729d9ce00d6",
      "b3dd5e651e2c4d65a1017eb21a6c0cb4",
      "b1c6df57429d4f5db71922d4bf2fc638",
      "004732c4415c41209975111ddeb89131",
      "5a7e0a144a5542b5937d7f2072f24422",
      "ddd85be505c74804b38f91215495f48a"
     ]
    },
    "id": "ASE7TWDlQENp",
    "outputId": "93af8636-e0ac-453c-e5e3-d91fd5026f7f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unet(\n",
       "  (encoder): ResNetEncoder(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): UnetDecoder(\n",
       "    (center): Identity()\n",
       "    (blocks): ModuleList(\n",
       "      (0): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(3072, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(768, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (segmentation_head): SegmentationHead(\n",
       "    (0): Conv2d(16, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Identity()\n",
       "    (2): Activation(\n",
       "      (activation): LogSoftmax(dim=None)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Подргружаем модель и задаём функцию потерь\n",
    "model = smp.Unet('resnet50', activation='logsoftmax', classes=2)#logsoftmax\n",
    "\n",
    "model.cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_criterion():\n",
    "    soft_dice = SoftDice()\n",
    "\n",
    "    def exp_dice(pred, target):\n",
    "        return 1 - soft_dice(torch.exp(pred[:, 1:]), target[:, 1:])\n",
    "\n",
    "    return exp_dice\n",
    "\n",
    "criterion = make_criterion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GP66jae0QK0k"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f-bmLtUuQ-LB",
    "outputId": "c688a1cd-8d92-4f75-c581-8eed472aaafe"
   },
   "outputs": [],
   "source": [
    "# Обучаем модель\n",
    "metric_functions = make_metrics()\n",
    "trainer = UnetTrainer(model, optimizer, criterion, 'cuda', metric_functions=metric_functions)\n",
    "summary = trainer.fit(train_loader, 40, val_loader=valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, './modelRes50_40_2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "9PZw7XACUngH",
    "outputId": "a313687d-d889-4516-945c-bc1c9cd05e85"
   },
   "outputs": [],
   "source": [
    "# Функция потерь\n",
    "plot_history(summary['loss_train'], summary['loss_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = torch.load('./modelRes50_40_2.pth')\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "rJ6-a76_JjI5",
    "outputId": "b2747ead-142f-4666-951f-80ed074c5e6d"
   },
   "outputs": [],
   "source": [
    "# Точность\n",
    "plot_history(summary['accuracy_train'], summary['accuracy_test'], \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "z2HIBtW4gYqe",
    "outputId": "2bc0e594-cb70-4256-ebf5-79148956b502"
   },
   "outputs": [],
   "source": [
    "# Полнота\n",
    "plot_history(summary['recall_train'], summary['recall_test'], \"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "REshXIEC5fay",
    "outputId": "08a7f73c-2e8e-4504-c450-ff3f9f711a74"
   },
   "outputs": [],
   "source": [
    "# Посмотрим на картинки предсказаний\n",
    "\n",
    "fig, axs = plt.subplots(2, 4, figsize=(16,8))\n",
    "fig.suptitle(f'Предскзаания модели {\" \"*105} Эталонная разметка', fontsize=14)\n",
    "\n",
    "for i, sample in zip(range(4), valid_dataset):\n",
    "    image = sample['image'].to(\"cuda\")\n",
    "    true_maks = sample['mask'].to(\"cuda\")\n",
    "\n",
    "    prediction = model.eval()(image.unsqueeze(dim=0))\n",
    "\n",
    "    image = (image.cpu() * 255).type(torch.uint8)\n",
    "    pred_mask = (torch.exp(prediction[0]) > 0.5).cpu()\n",
    "\n",
    "    image_with_mask = draw_segmentation_masks(image, pred_mask)\n",
    "    image_with_mask = np.moveaxis(image_with_mask.cpu().numpy(), 0, -1)\n",
    "\n",
    "    axs[i // 2, (i % 2)].imshow(image_with_mask)\n",
    "    axs[i // 2, (i % 2)].axis('off')\n",
    "\n",
    "\n",
    "    image_with_mask = draw_segmentation_masks(image, true_maks.type(torch.bool))\n",
    "    image_with_mask = np.moveaxis(image_with_mask.cpu().numpy(), 0, -1)\n",
    "    axs[i // 2, (i % 2)+2].imshow(image_with_mask)\n",
    "    axs[i // 2, (i % 2)+2].axis('off')\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.88)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestEyeDataset(Dataset):\n",
    "    def __init__(self, data_folder: str, transform = None):\n",
    "#         self.class_ids = {\"vessel\": 1}\n",
    "\n",
    "        self.data_folder = data_folder\n",
    "        self.transform = transform\n",
    "        self._image_files = glob.glob(f\"{data_folder}/eye_test/*.png\")\n",
    "\n",
    "    @staticmethod\n",
    "    def read_image(path: str) -> np.ndarray:\n",
    "        image = cv2.imread(str(path), cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = np.array(image / 255, dtype=np.float32)\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        # Достаём имя файла по индексу\n",
    "        image_path = self._image_files[idx]\n",
    "\n",
    "        image = self.read_image(image_path)\n",
    "\n",
    "        sample = {'image': image}#,                  'mask': mask}\n",
    "\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(**sample)\n",
    "\n",
    "        return sample, image_path\n",
    " \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._image_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 512 \n",
    "test_list = [A.LongestMaxSize(size, interpolation=cv2.INTER_CUBIC),\n",
    "              A.PadIfNeeded(size, size),\n",
    "              ToTensorV2(transpose_mask=True)]\n",
    "\n",
    "test_transforms = A.Compose(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TestEyeDataset('data', transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 301/301 [01:32<00:00,  3.26it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, ind in tqdm(test_data):\n",
    "    image = i['image'].to(\"cuda\")\n",
    "    \n",
    "    prediction = model.eval()(image.unsqueeze(dim=0))\n",
    "\n",
    "    mask = (torch.exp(prediction[0]) > 0.5).cpu()\n",
    "    mask = (mask.cpu() * 255).type(torch.uint8)\n",
    "    mask = np.moveaxis(mask.cpu().numpy(), 0, -1)\n",
    "#     print(mask.shape)\n",
    "    mask = mask[:,:,1]\n",
    "#     print(mask.shape)\n",
    "    mask = cv2.resize(mask, (1624,1232), interpolation=cv2.INTER_LINEAR)\n",
    "#     print(mask.shape)\n",
    "    _, mask = cv2.threshold(mask, 100,255,cv2.THRESH_BINARY)\n",
    "#     print(mask.shape)\n",
    "    name_image = ind.split('\\\\')[-1]\n",
    "    cv2.imwrite(f'data/eye_pred/{name_image}', mask.astype('uint8'), [int(cv2.IMWRITE_PNG_COMPRESSION), 9])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(image=mask)\n",
    "\n",
    "# mask *= 255\n",
    "# mask = mask.astype(np.uint8)\n",
    "# result_m = Image.fromarray((mask).astype(np.uint8))\n",
    "# result_m.save(f'data/m1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.dtype"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "004732c4415c41209975111ddeb89131": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0e92432f54b54b9abc2ba7636fbf1f77": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "327c8a838df5445d8a8a5729d9ce00d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e5fd46dd3d04e91a9eb09c157a64e75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9fd271a089ed42cabb20e0040d043e72",
       "IPY_MODEL_544f60a9a0674e4690b9cbbeb67e384e",
       "IPY_MODEL_e92928dc60d34de087a5b29c86eadc3d"
      ],
      "layout": "IPY_MODEL_0e92432f54b54b9abc2ba7636fbf1f77"
     }
    },
    "544f60a9a0674e4690b9cbbeb67e384e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1c6df57429d4f5db71922d4bf2fc638",
      "max": 102502400,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_004732c4415c41209975111ddeb89131",
      "value": 102502400
     }
    },
    "5a7e0a144a5542b5937d7f2072f24422": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9fd271a089ed42cabb20e0040d043e72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_327c8a838df5445d8a8a5729d9ce00d6",
      "placeholder": "​",
      "style": "IPY_MODEL_b3dd5e651e2c4d65a1017eb21a6c0cb4",
      "value": "100%"
     }
    },
    "b1c6df57429d4f5db71922d4bf2fc638": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b3dd5e651e2c4d65a1017eb21a6c0cb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ddd85be505c74804b38f91215495f48a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e92928dc60d34de087a5b29c86eadc3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a7e0a144a5542b5937d7f2072f24422",
      "placeholder": "​",
      "style": "IPY_MODEL_ddd85be505c74804b38f91215495f48a",
      "value": " 97.8M/97.8M [00:01&lt;00:00, 95.9MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
